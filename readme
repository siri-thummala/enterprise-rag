## Enterprise RAG Intelligence Hub
An AI-powered Document Research Assistant built with Python, LangChain, and Google Gemini 2.5.

## ğŸ§  Project Concept
 An end-to-end Retrieval-Augmented Generation (RAG) application that enables users to upload multiple PDF documents and ask natural-language questions over their content. The system performs semantic search using vector embeddings and generates grounded answers with source attribution via a Streamlit interface.It solves the problem of "AI Hallucinations" by forcing the model to cite your private data.

## ğŸ›  The Tech Stack 
- Language Model: gemini-2.5-flash-lite â€” Chosen for its massive context window and high-speed reasoning.

- Embeddings: text-embedding-004 â€” Converts human text into high-dimensional math (3,072 dimensions) so the AI can "calculate" the meaning of sentences.

- Vector Database: ChromaDB â€” A high-performance "filing cabinet" that stores your PDF data in a searchable mathematical format.

- Orchestration: LangChain â€” The framework that connects the PDF loader, the database, and the AI model into a single pipeline.



## ğŸ— System 
- The project is split into two distinct phases:

## 1. The Ingestion Pipeline (ingest.py)

- This is the "Learning Phase".
- Load: Reads the raw PDF file.
- Split: Breaks the text into 1,000-character chunks so the AI doesn't get overwhelmed.
- Embed: Turns those chunks into vectors (numbers) using Google's embedding model.
- Store: Saves those vectors into the ./chroma_db folder for permanent access.

## 2. The Query Pipeline (app.py)

- This is the "Answering Phase".
- Search: When you ask a question, the system searches the ChromaDB for the most relevant text chunks.
- Context: It grabs the specific paragraphs from your PDF that match your question.
- Augment: It sends your question + the PDF paragraphs to Gemini.
- Answer: Gemini reads the provided text and answers your question based only on that info.



## System Architecture

PDFs
 â†“
Ingestion Pipeline (ingest.py)
 â†“
Vector Embeddings + Metadata
 â†“
ChromaDB (Persistent Vector Store)
 â†“
Streamlit UI (app.py)
 â†“
User Query â†’ Retrieval â†’ LLM â†’ Answer + Sources



## Project Structure
enterprise-rag/
â”‚
â”œâ”€â”€ ingest.py            # Offline ingestion pipeline
â”œâ”€â”€ app.py               # Streamlit application
â”œâ”€â”€ check_setup.py       # Gemini API connectivity test
â”‚
â”œâ”€â”€ pdfs/                # Uploaded PDF documents
â”‚
â”œâ”€â”€ chroma_db/            # Persistent vector database
â”‚
â”œâ”€â”€ venv/                # Python virtual environment
â”œâ”€â”€ .env                 # API keys
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md




## ğŸš€ Features

- Multi-PDF ingestion
 * Upload and index multiple PDF documents
 * Page-level parsing with chunking for accurate retrieval

- Semantic search
 * Vector-based similarity search using Gemini embeddings
 * Persistent storage with ChromaDB

- Retrieval-Augmented Generation
 * Relevant document chunks are retrieved and passed as context to the LLM
 * Answers are generated strictly from retrieved content

- Source attribution
 * Each response includes the PDF name and page number
 * Improves transparency and trust

- Streamlit UI
 * Upload PDFs directly from the browser
 * Ask questions interactively without restarting the app

- Hallucination control
 * If the answer is not present in the retrieved context, the system responds with â€œI donâ€™t knowâ€




## Setup Instructions
- Create & Activate Virtual Environment
 * python3 -m venv venv
   source venv/bin/activate

- Install Dependencies
 * pip install streamlit langchain langchain-google-genai langchain-chroma chromadb python-dotenv

- Create a .env file in the project root:
 * GOOGLE_API_KEY=your_gemini_api_key_here



## Usage
ğŸ”¹ Step 1: Ingest PDFs (Offline)
- python ingest.py
ğŸ”¹ Step 2: Run the Streamlit App
- streamlit run app.py
